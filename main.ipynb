{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be37a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from .env\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fac9a",
   "metadata": {},
   "source": [
    "* Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aca6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de84779",
   "metadata": {},
   "source": [
    "* Load the PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0fc2967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_pdf_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    texts = []\n",
    "    for page in doc:\n",
    "        texts.append(page.get_text())\n",
    "    return \"\\n\".join(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e0804",
   "metadata": {},
   "source": [
    "* Generate Clean text\n",
    "* Used OpenAI so that during cleaning the semantic meaning does not get affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_with_openai(raw_text):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "The following is raw extracted text from a Bangla literature PDF. \n",
    "Please clean it up into readable paragraphs, fix broken sentences, and keep the semantic meaning intact.\n",
    "\n",
    "Raw text:\n",
    "{raw_text}\n",
    "\n",
    "Cleaned text:\n",
    "\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "pdf_path = \"data/HSC26_Bangla_1st_Paper.pdf\"\n",
    "raw_text = extract_raw_pdf_text(pdf_path)\n",
    "cleaned_text = clean_text_with_openai(raw_text)\n",
    "\n",
    "print(cleaned_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65ff98",
   "metadata": {},
   "source": [
    "* Creating Chunks\n",
    "* Also used OpenAI to hold the meaning of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_into_semantic_chunks(cleaned_text, desired_chunk_count=20):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. The following is a cleaned Bangla literature text.\n",
    "\n",
    "Your task is to split the text into around {desired_chunk_count} semantically meaningful chunks. \n",
    "Each chunk should represent a complete idea, paragraph, or topic without breaking the meaning.\n",
    "Separate each chunk using this delimiter:\n",
    "====CHUNK====\n",
    "\n",
    "Text:\n",
    "{cleaned_text[:3500]}  # You can loop this if text is longer\n",
    "\n",
    "Semantic Chunks:\n",
    "\"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",  \n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    chunks_text = response.choices[0].message.content.strip()\n",
    "    chunks = [chunk.strip() for chunk in chunks_text.split(\"====CHUNK====\") if chunk.strip()]\n",
    "    return chunks\n",
    "\n",
    "semantic_chunks = split_into_semantic_chunks(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28458f7",
   "metadata": {},
   "source": [
    "* Embedding the data \n",
    "* Creating vector store [ Long Term Memory ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ac5568",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vector_store = FAISS.from_texts(chunks, embedding)\n",
    "vector_store.save_local(\"index/faiss_index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b0a9c",
   "metadata": {},
   "source": [
    "* Retriever Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b579100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def retrieve_similar_chunks(user_query, top_k=3):\n",
    "    \n",
    "    embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "    vector_store = FAISS.load_local(\n",
    "        \"index/faiss_index\",\n",
    "        embedding,\n",
    "        allow_dangerous_deserialization=True  \n",
    "    )\n",
    "\n",
    "    results = vector_store.similarity_search(user_query, k=top_k)\n",
    "    return [doc.page_content for doc in results]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec943f38",
   "metadata": {},
   "source": [
    "* Short Term Memory\n",
    "* A simple list that will hold the last 5 conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f545dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "short_term_memory = []\n",
    "def update_short_term_memory(user_question, bot_answer, max_len=5):\n",
    "    global short_term_memory\n",
    "    if len(short_term_memory) >= max_len:\n",
    "        short_term_memory.pop(0)  \n",
    "    short_term_memory.append(f\"User: {user_question}\\nBot: {bot_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab124d9",
   "metadata": {},
   "source": [
    "* Generating answer \n",
    "* Used OpenAI to generate ground truth using retrieval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_answer(query, short_term_memory, top_k=3):\n",
    "    relevant_texts = retrieve_similar_chunks(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join(relevant_texts)\n",
    "    short_memory_context = \"\\n\".join(short_term_memory) if short_term_memory else \"No prior chat history.\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a knowledgeable assistant for Bangla literature Q&A.\n",
    "Answer the user's question based on the context from a textbook.\n",
    "Use both retrieved document and chat history if needed.\n",
    "Do not asnwer if you don't know the exact answer.\n",
    "\n",
    "Chat History:\n",
    "{short_memory_context}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question:\n",
    "{query}\n",
    "\n",
    "Answer very briefly, in one short phrase or word, directly and factually.\n",
    "\"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You're a helpful and precise Bangla literature expert.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    answer = response['choices'][0]['message']['content'].strip()\n",
    "    update_short_term_memory(query, answer)\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f484e1",
   "metadata": {},
   "source": [
    "* Taking User Query -> Generate Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3de012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\n",
      "Bot: ১৫ বছর\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_query = \"বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\"\n",
    "    answer = generate_answer(user_query, short_term_memory)\n",
    "    print(f\"User: {user_query}\\nBot: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b594da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c9bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ee94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c5d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
